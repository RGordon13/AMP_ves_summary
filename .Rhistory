title = "")+
facet_nested(rows = vars(network, npz_id, Dep_ID),
nest_line = element_line(),
scales = "free_y") +
theme(
axis.text.x = element_text(face = "bold"),
strip.background = element_blank(),
strip.text.y = element_text(face = "bold")
)
ggplot(data = light_means_tot,
aes(x = light_reg,
y = mean_light_adj)) +
geom_hline(yintercept = 0,
linetype = 3, # dotted line
colour = "black") +
# linerange for inside error bars
geom_linerange(data = light_means_ins,
aes(ymin = sd_lower,
ymax = sd_upper,
group = network),
color = "black",
show.legend = FALSE) +
# bar for inside vessels
geom_col(data = light_means_ins,
aes(x = light_reg,
y = mean_light_adj,
# conditional fill based on lower SD value
# "if mean +/- SD is above 0, 'over'
fill = ifelse(sd_lower >= 0, 'over',
ifelse(sd_upper <=0, 'under', 'center'))
),
show.legend = FALSE
) +
#translucent bar for total vessels
geom_col(data = light_means_tot,
aes(x = light_reg,
y = mean_light_adj),
show.legend = FALSE,
alpha = 0.25,
fill = "grey50")+
# error bars for total vessels
geom_linerange(data = light_means_tot,
aes(ymin = sd_lower,
ymax = sd_upper,
group = network),
color = "grey50",
# size = 0.1,
show.legend = FALSE) +
scale_fill_manual(values = c("over" = "deepskyblue",
"under" = "orange",
"center" = "grey75"))+
# scale_x_continuous(breaks = seq(0,23,2))+
labs(y = "Mean-adjusted average N vessels / hour",
x = "Light Regime",
title = "")+
facet_nested(rows = vars(network),
nest_line = element_line(),
scales = "free_y") +
theme(
axis.text.x = element_text(face = "bold"),
strip.background = element_blank(),
strip.text.y = element_text(face = "bold")
)
View(light_means_tot)
# summarize means & SD for plots
light_means_tot <- light_mean_adj_tot |>
# group to light level per deployment
group_by(network, light_reg) |>
# get mean of mean-adjusted ounts per hour
summarise(mean_light_adj = mean(tot_light_adj, na.rm =TRUE)) |>
# get lower & upper bounds for SD around mean of adjusted counts
mutate(sd_lower = mean_light_adj - 0.5*(sd(mean_light_adj)),
sd_upper = mean_light_adj + 0.5*(sd(mean_light_adj)))
ggplot(data = light_mean_adj_tot,
aes(x = npz_id, y = tot_light_adj, color = light_reg)) +
geom_boxplot()
ggplot(data = light_means_tot,
aes(x = network, y = tot_light_adj, color = light_reg)) +
geom_boxplot()
ggplot(data = light_means_tot,
aes(x = network, y = mean_light_adj, color = light_reg)) +
geom_boxplot()
ggplot(data = light_means_tot,
aes(x = network, y = mean_light_adj, color = light_reg)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper))
ggplot(data = light_means_tot,
aes(x = light_reg, y = mean_light_adj)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper)) +
facet_grid(rows = vars(network))
View(light_means_tot)
ggplot(data = light_means_tot,
aes(x = light_reg, y = mean_light_adj)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper)) +
facet_grid(rows = vars(network),
scales = "free_y")
ggplot(data = light_means_tot,
aes(x = light_reg, y = mean_light_adj)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper)) +
hline(aes(y = 0))+
facet_grid(rows = vars(network),
scales = "free_y")
ggplot(data = light_means_tot,
aes(x = light_reg, y = mean_light_adj)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper)) +
geom_hline(aes(y = 0))+
facet_grid(rows = vars(network),
scales = "free_y")
ggplot(data = light_means_tot,
aes(x = light_reg, y = mean_light_adj)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper)) +
geom_hline(yintercept = 0)+
facet_grid(rows = vars(network),
scales = "free_y")
ggplot(data = light_means_tot,
aes(x = light_reg, y = mean_light_adj)) +
geom_pointrange(aes(ymin = sd_lower, ymax=sd_upper)) +
geom_hline(yintercept = 0,
linetype = 3)+
facet_grid(rows = vars(network),
scales = "free_y")
View(dunntable_tot_northwest)
View(dunntable_light_tot_northwest)
View(dunn_light_tot_northwest)
dunn_light_tot_northwest$res
View(week_aov_tmpeast)
summary(week_aov_tmpeast)
summary(week_aov_northwest)
summary(week_aov_swest)
View(week_aov_northwest)
wkly_mean_adj_tot |>
filter(network == "Northwest")
View(wkly_mean_adj_tot_northwest)
View(wkly_mean_adj_tot_northwest)
summary(week_ins_aov_tmpeast)
summary(week_ins_aov_northwest)
summary(week_ins_aov_swest)
summary(week_man_aov_tmpeast
)
summary(week_man_aov_northwest)
summary(week_man_aov_swest)
View(dunntable_tot_tmpeast)
View(dunntable_light_man_tmpeast)
View(dunntable_light_tot_tmpeast)
View(dunntable_light_tot_swest)
View(dunntable_light_tot_tmpeast)
View(dunntable_light_tot_swest)
# NOAA-approved tidyverse
tidyverse_short<-c("broom","cli","crayon","dbplyr","dplyr","dtplyr","forcats","ggplot2","googledrive","googlesheets4","hms","httr","jsonlite","lubridate","magrittr","modelr","pillar","purrr","readr","readxl","reprex","rlang","rstudioapi","rvest","stringr","tibble","tidyr","xml2")
lapply(tidyverse_short, require, character.only = TRUE)
# source helper file with various data wrangling functions
source("code/AMP_summary_ves_funs.R")
# set higher digits for using sci notation to get around importing weird date format from Excel
options(scipen = 13)
#### Load in data ####
# Assign user-defined inputs ----------------------------------------------
# Since server folders are in a standard structure, use parent folder to get list of all deployments
dep_names <- list.dirs(tk_choose.dir(caption = "Select parent dir for all deployment folders"), recursive = FALSE, full.names = FALSE)
# select the deployment(s) to be plotted
dep_list <- dlg_list(title = "Select deployments to plot", choices = dep_names, multiple = TRUE)$res |>
str_sub(start = 16)
# apply getDeploymentInfo() from AMP_pkgs_funs.R to each deployment
#   prompts user for site name, start/end date, and time zones
dep_info <- dep_list |>
map(~getDeploymentInfo(.)) |>
set_names(dep_list)
# For each deployment in the dep_info list, load in hourly presence table & compiled selection table
hp_og <- dep_info |>
map(~read_csv(choose.files(caption = paste0({.}$site_id, {.}$dep_id, " Hourly Presence sheet .csv"))))|>
# use imap() to get info based on index of each iteration
# in this case, we want the name of the list element, designated as ".y"
imap(~mutate(., Dep_ID = .y))
# For each deployment in the dep_info list, load in compiled selection table
selns_og <- dep_info |>
map(~read_delim(choose.files(caption = paste0({.}$site_id, {.}$dep_id, " Complied seln table .txt"))))|>
# use imap() to get info based on index of each iteration
# in this case, we want the name of the list element, designated as ".y"
imap(~mutate(., Dep_ID = .y))
# For each deployment in the dep_info list, load in I-O excel files
ins_out_og <- dep_info |>
map(~read_csv(choose.files(caption = paste0({.}$site_id, {.}$dep_id, " I-O table"))))|>
# use imap() to get info based on index of each iteration
# in this case, we want the name of the list element, designated as ".y"
imap(~mutate(., Dep_ID = .y))
#### Add some columns, update time zone ####
# making new_cols placeholders to add into original df if they don't exist
new_cols <- c("Maneuver" = 0,"Transit" = 0, "Not_Assigned" = 0)
hp_allcols <- hp_og |>
# add columns for Transit and Maneuver if they don't exist
map(~add_column(., !!!new_cols[!names(new_cols) %in% names(.)])
)
# Hourly presence
hp_data <- hp_allcols |>
map(~rename(.,
any_of(c("TR" = "Transit",
"M" = "Maneuver",
"Dep" = "Dep_ID")))) |>
map(~relocate(.,
TR, .after = last_col())) |>
map(~relocate(.,
Not_Assigned, .after = last_col())) |>
map(~relocate(.,
M, .after = last_col())) |>
map2(.y = dep_info,
~mutate(.x,
Site_ID = {.y}$site_id,
Dep_ID = {.y}$dep_id,
Total_Vessels = rowSums(across(TR:M)),
# create y/n column for vessel presence
ves_yn = ifelse(Total_Vessels == 0, "N", "Y"),
# pull hour as time object along with date
Hr_time = paste(Begin_Date, Begin_Hour, ":00"),
# get components of date-time object and assign time zone
Hr_time = parse_date_time(Hr_time, "ymd H:M", tz = {.y}$tz_files),
# change to local time zone
Hr_local = with_tz(Hr_time, tzone = {.y}$tz_local),
# pull out new hour and date in local time
Begin_Hour_loc = as.numeric(hour(Hr_local)),
Begin_Date_loc = date(Hr_local),
# add weekday column
Weekday = weekdays(Begin_Date_loc))) |>
bind_rows()
# Create CSV to make data input easier. Maybe eventually add some logic in script to bypass generating this?
write_csv(hp_data, "Manuscript_revisions_202510/output/total_ves_hp_by_site.csv")
# reshape selection tables to plot duration
selns_data <- selns_og |>
map(~rename(., "Dep" = "Dep_ID")) |>
map2(.y = dep_info,
~mutate(.x,
Begin_Date = ymd(Begin_Date),
Begin_file_date = ymd(Begin_file_date),
Site_ID = {.y}$site_id,
# Dep = {.y}$dep_id,
# re-code anything with a maneuver as "Maneuver" (e.g., "Maneuver+CPA" = "Maneuver")
Behavior = gsub(pattern = ".*Maneuver.*", replacement = "Maneuver", x = Behavior),
# re-code all transit as "Transit" (e.g., "TransitA" = "Transit")
# important that this happens AFTER the Maneuver line above so that "TransitAManeuver" doesn't get re-coded
Behavior = gsub(pattern = ".*Transit.*", replacement = "Transit", x = Behavior),
Behavior = gsub(pattern = "CPAManeuver", replacement = "Maneuver", x = Behavior),
Behavior = gsub(pattern = "CPA", replacement = "Transit", x = Behavior),
Behavior = gsub(pattern = " ", replacement = "", x = Behavior),
DeltaHours = Delta_Time_s/3600,
# Total_Vessels = rowSums(across(c(TR,M))),
# pull hour as time object along with date
Hr_time = paste(Begin_Date, Begin_Clock),
# get components of date-time object and assign time zone
Hr_time = parse_date_time(Hr_time, "y/m/d H:M:S", tz = {.y}$tz_files),
# change to local time zone
Hr_local = with_tz(Hr_time, tzone = {.y}$tz_local),
# pull out new hour and date in local time
Begin_Hour_loc = as.numeric(hour(Hr_local)),
Begin_Date_loc = date(Hr_local),
# add weekday column
Weekday = weekdays(Begin_Date_loc))) |>
bind_rows()
# write csv for easier input later
write.csv(selns_data, "Manuscript_revisions_202510/output/total_ves_selns_data_by_site.csv")
ins_out_data <- ins_out_og |>
map(~select(., Filename, Date, Selection, used, pins_sm, pins_med, pins_lg, pins_ovrll, Dep_ID)) |>
map(~mutate(., Date = as.numeric(Date),
Date = as.character(Date),
pins_sm = as.numeric(pins_sm),
pins_med = as.numeric(pins_med),
pins_lg = as.numeric(pins_lg),
pins_ovrll = as.numeric(pins_ovrll))) |>
bind_rows()
# For each deployment in the dep_info list, load in I-O excel files
ins_out_og <- dep_info |>
map(~read_csv(choose.files(caption = paste0({.}$site_id, {.}$dep_id, " I-O table"))))|>
# use imap() to get info based on index of each iteration
# in this case, we want the name of the list element, designated as ".y"
imap(~mutate(., Dep_ID = .y))
ins_out_data <- ins_out_og |>
map(~select(., Filename, Date, Selection, used, pins_sm, pins_med, pins_lg, pins_ovrll, Dep_ID)) |>
map(~mutate(., Date = as.numeric(Date),
Date = as.character(Date),
pins_sm = as.numeric(pins_sm),
pins_med = as.numeric(pins_med),
pins_lg = as.numeric(pins_lg),
pins_ovrll = as.numeric(pins_ovrll))) |>
bind_rows()
# write csv for easier input later
write.csv(ins_out_data, "output/total_ves_ins_out_by_site.csv")
# Small vessels
ins_vessels_small <- ins_out_data |>
filter(used == 1) |>
filter(pins_sm >= 0.75) |>
mutate(begin_file_date = ymd(str_sub(Date, 1,6)),
seln_num = as.numeric(gsub(pattern = "S", replacement = "", x = Selection)))|>
left_join(selns_data,
by = c("Dep_ID" = "Dep",
"seln_num" = "Selection",
"begin_file_date" = "Begin_file_date"))
# Small or medium vessels
ins_vessels_smmed <- ins_out_data |>
filter(used == 1) |>
filter(pins_ovrll >= 0.75) |>
mutate(begin_file_date = ymd(str_sub(Date, 1,6)),
seln_num = as.numeric(gsub(pattern = "S", replacement = "", x = Selection)))|>
left_join(selns_data,
by = c("Dep_ID" = "Dep",
"seln_num" = "Selection",
"begin_file_date" = "Begin_file_date"))
# get counts per site per date per hour for inside vessels
# Small vessels
ins_hp_small <- inside_tables_to_hp(ins_table = ins_vessels_small) |>
rename("man_inside_small" = "man_inside",
"trans_inside_small" = "trans_inside",
"total_inside_small" = "total_inside")
ins_table = ins_vessels_small
hr_tally <- ins_table |>
mutate(Behavior = replace_na(Behavior, "Not_Assigned"))
hr_tally <- ins_table |>
mutate(Behavior = replace_na(Behavior, "Not_Assigned")) |>
group_by(Begin_Date_loc, Behavior, Begin_Hour_loc, Dep_ID)
hr_tally <- ins_table |>
mutate(Behavior = replace_na(Behavior, "Not_Assigned")) |>
group_by(Begin_Date_loc, Behavior, Begin_Hour_loc, Dep_ID) |>
count()
hr_tally <- ins_table |>
mutate(Behavior = replace_na(Behavior, "Not_Assigned")) |>
group_by(Begin_Date_loc, Behavior, Begin_Hour_loc, Dep_ID) |>
count() |>
pivot_wider(names_from = Behavior,
values_from = n,
names_expand = TRUE,
names_prefix = "ins_",
values_fill = 0)
hr_tally <- ins_table |>
mutate(Behavior = replace_na(Behavior, "Not_Assigned")) |>
group_by(Begin_Date_loc, Behavior, Begin_Hour_loc, Dep_ID) |>
count() |>
pivot_wider(names_from = Behavior,
values_from = n,
names_expand = TRUE,
names_prefix = "ins_",
values_fill = 0) |> # expand to include a column for all possible levels
rename("trans_inside" = "ins_Transit",
"man_inside" = "ins_Maneuver")
hr_tally <- ins_table |>
mutate(Behavior = replace_na(Behavior, "Not_Assigned")) |>
group_by(Begin_Date_loc, Behavior, Begin_Hour_loc, Dep_ID) |>
count() |>
pivot_wider(names_from = Behavior,
values_from = n,
names_expand = TRUE,
names_prefix = "ins_",
values_fill = 0) |> # expand to include a column for all possible levels
rename("trans_inside" = "ins_Transit",
"man_inside" = "ins_Maneuver") |>
mutate(total_inside = rowSums(across(c(trans_inside, man_inside))))
max(hr_tally$Begin_Date_loc)
min(hr_tally$Begin_Date_loc)
View(hr_tally)
View(ins_vessels_small)
ins_vessels_small[which(is.ns(ins_vessels_small$Begin_Date_loc)),]
ins_vessels_small[which(is.na(ins_vessels_small$Begin_Date_loc)),]
selns_to_check <- ins_vessels_small[which(is.na(ins_vessels_small$Begin_Date_loc)),]
View(selns_to_check)
# Small or medium vessels
ins_hp_smmed <- inside_tables_to_hp(ins_table = ins_vessels_smmed)
selns_to_check_smmed <- ins_vessels_smmed[which(is.na(ins_vessels_smmed$Begin_Date_loc)),]
View(selns_to_check_smmed)
View(selns_to_check)
# For each deployment in the dep_info list, load in I-O excel files
ins_out_og <- dep_info |>
map(~read_csv(choose.files(caption = paste0({.}$site_id, {.}$dep_id, " I-O table"))))|>
# use imap() to get info based on index of each iteration
# in this case, we want the name of the list element, designated as ".y"
imap(~mutate(., Dep_ID = .y))
ins_out_data <- ins_out_og |>
map(~select(., Filename, Date, Selection, used, pins_sm, pins_med, pins_lg, pins_ovrll, Dep_ID)) |>
map(~mutate(., Date = as.numeric(Date),
Date = as.character(Date),
pins_sm = as.numeric(pins_sm),
pins_med = as.numeric(pins_med),
pins_lg = as.numeric(pins_lg),
pins_ovrll = as.numeric(pins_ovrll))) |>
bind_rows()
# write csv for easier input later
write.csv(ins_out_data, "output/total_ves_ins_out_by_site.csv")
# Small vessels
ins_vessels_small <- ins_out_data |>
filter(used == 1) |>
filter(pins_sm >= 0.75) |>
mutate(begin_file_date = ymd(str_sub(Date, 1,6)),
seln_num = as.numeric(gsub(pattern = "S", replacement = "", x = Selection)))|>
left_join(selns_data,
by = c("Dep_ID" = "Dep",
"seln_num" = "Selection",
"begin_file_date" = "Begin_file_date"))
# Small or medium vessels
ins_vessels_smmed <- ins_out_data |>
filter(used == 1) |>
filter(pins_ovrll >= 0.75) |>
mutate(begin_file_date = ymd(str_sub(Date, 1,6)),
seln_num = as.numeric(gsub(pattern = "S", replacement = "", x = Selection)))|>
left_join(selns_data,
by = c("Dep_ID" = "Dep",
"seln_num" = "Selection",
"begin_file_date" = "Begin_file_date"))
# get counts per site per date per hour for inside vessels
# Small vessels
ins_hp_small <- inside_tables_to_hp(ins_table = ins_vessels_small) |>
rename("man_inside_small" = "man_inside",
"trans_inside_small" = "trans_inside",
"total_inside_small" = "total_inside")
# Small or medium vessels
ins_hp_smmed <- inside_tables_to_hp(ins_table = ins_vessels_smmed)
# write csv for easier input later
write.csv(ins_out_data, "Manuscript_revisions_202510/output/total_ves_ins_out_by_site.csv")
# Small vessels
ins_vessels_small <- ins_out_data |>
filter(used == 1) |>
filter(pins_sm >= 0.75) |>
mutate(begin_file_date = ymd(str_sub(Date, 1,6)),
seln_num = as.numeric(gsub(pattern = "S", replacement = "", x = Selection)))|>
left_join(selns_data,
by = c("Dep_ID" = "Dep",
"seln_num" = "Selection",
"begin_file_date" = "Begin_file_date"))
# Small or medium vessels
ins_vessels_smmed <- ins_out_data |>
filter(used == 1) |>
filter(pins_ovrll >= 0.75) |>
mutate(begin_file_date = ymd(str_sub(Date, 1,6)),
seln_num = as.numeric(gsub(pattern = "S", replacement = "", x = Selection)))|>
left_join(selns_data,
by = c("Dep_ID" = "Dep",
"seln_num" = "Selection",
"begin_file_date" = "Begin_file_date"))
# get counts per site per date per hour for inside vessels
# Small vessels
ins_hp_small <- inside_tables_to_hp(ins_table = ins_vessels_small) |>
rename("man_inside_small" = "man_inside",
"trans_inside_small" = "trans_inside",
"total_inside_small" = "total_inside")
# Small or medium vessels
ins_hp_smmed <- inside_tables_to_hp(ins_table = ins_vessels_smmed)
# join inside vessels to total vessels
total_ins_hp <- hp_data |>
left_join(ins_hp_small,
by = c("Dep" = "Dep_ID",
"Begin_Date_loc" = "Begin_Date_loc",
"Begin_Hour_loc" = "Begin_Hour_loc")) |>
left_join(ins_hp_smmed,
by = c("Dep" = "Dep_ID",
"Begin_Date_loc" = "Begin_Date_loc",
"Begin_Hour_loc" = "Begin_Hour_loc"))
# save CSV with all sites' hourly presence (total & inside NPZ)
write.csv(total_ins_hp, "Manuscript_revisions_202510/output/hourly_pres_allsites_local.csv")
tidyverse_short<-c("broom","cli","crayon","dbplyr","dplyr","dtplyr","forcats","ggplot2","googledrive","googlesheets4","hms","httr","jsonlite","lubridate","magrittr","modelr","pillar","purrr","readr","readxl","reprex","rlang","rstudioapi","rvest","stringr","tibble","tidyr","xml2")
lapply(tidyverse_short, require, character.only = TRUE)
library(scales)
# library(ggalt)
# library(ggsci)
library(ggdist)
library(ggh4x) # nested faceting
library(ggpubr) # additional options for publication plots
library(suncalc) # sunrise & sunset calculations
library(geosphere)
library(ggpattern)
library(MuMIn) # AICc values
library(fmsb) # for pairwise fisher testing
library(performance) # check models
library(suntools)
theme_set(theme_bw(
base_size = 14
))
# source helper file
source("code/AMP_summary_ves_funs.R")
source("code/AMP_summary_ves_funs.R")
library(tcltk2)
library(tcltk2)
library(svDialogs)
source("code/AMP_summary_ves_funs.R")
---
title: "AMP_ves_summary_plots"
```{r load libraries and helper script, include=FALSE, message=FALSE, warning=FALSE, echo=FALSE}
source("code/AMP_summary_ves_funs.R")
all_sites_hp <- read_csv("data/All_sites_tables/hourly_pres_allsites_local.csv")
all_sites_hp <- read_csv("data/All_sites_tables/hourly_pres_allsites_local.csv")
tidyverse_short<-c("broom","cli","crayon","dbplyr","dplyr","dtplyr","forcats","ggplot2","googledrive","googlesheets4","hms","httr","jsonlite","lubridate","magrittr","modelr","pillar","purrr","readr","readxl","reprex","rlang","rstudioapi","rvest","stringr","tibble","tidyr","xml2")
lapply(tidyverse_short, require, character.only = TRUE)
library(scales)
# library(ggalt)
# library(ggsci)
library(ggdist)
library(ggh4x) # nested faceting
library(ggpubr) # additional options for publication plots
library(suncalc) # sunrise & sunset calculations
library(geosphere)
library(ggpattern)
library(MuMIn) # AICc values
library(fmsb) # for pairwise fisher testing
library(performance) # check models
library(suntools)
theme_set(theme_bw(
base_size = 14
))
source("code/AMP_summary_ves_funs.R")
source("Manuscript_revisions_202510/code/AMP_summary_ves_funs.R")
source("Manuscript_revisions_202510/code/AMP_summary_ves_funs.R")
getwd()
source("C:/Users/rochelle.gordon/Documents/GitHub/AMP_ves_summary/Manuscript_revisions_202510/code/AMP_summary_ves_funs.R")
setwd(C:/Users/rochelle.gordon/Documents/GitHub/AMP_ves_summary/Manuscript_revisions_202510)
all_sites_hp <- read_csv("Manuscript_revisions_202510/data/All_sites_tables/hourly_pres_allsites_local.csv")
setwd(C:Users/rochelle.gordon/Documents/GitHub/AMP_ves_summary/Manuscript_revisions_202510)
> setwd(C:/Users/rochelle.gordon/Documents/GitHub/AMP_ves_summary/Manuscript_revisions_202510)
> setwd("C:Users/rochelle.gordon/Documents/GitHub/AMP_ves_summary/Manuscript_revisions_202510")
> setwd('C:Users/rochelle.gordon/Documents/GitHub/AMP_ves_summary/Manuscript_revisions_202510')
all_sites_hp <- read_csv("Manuscript_revisions_202510/data/All_sites_tables/hourly_pres_allsites_local.csv")
getwd()
